{
    "content": "<?xml version=\"1.0\" encoding=\"UTF-8\"?> <feed xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns=\"http://www.w3.org/2005/Atom\"> <title>International Conference on Auditory Display</title> <link href=\"http://hdl.handle.net/1853/44394\" rel=\"alternate\"/> <subtitle>Annual Conferece of the International Community for Auditory Display</subtitle> <id>http://hdl.handle.net/1853/44394</id> <updated>2015-03-12T19:31:00Z</updated> <dc:date>2015-03-12T19:31:00Z</dc:date> <entry> <title>A sonification of Kepler space telescope star data</title> <link href=\"http://hdl.handle.net/1853/44451\" rel=\"alternate\"/> <author> <name>Winton, Riley J.</name> </author> <author> <name>Gable, Thomas M.</name> </author> <author> <name>Schuett, Jonathan</name> </author> <author> <name>Walker, Bruce N.</name> </author> <id>http://hdl.handle.net/1853/44451</id> <updated>2012-08-03T18:32:56Z</updated> <published>2012-06-01T00:00:00Z</published> <summary type=\"text\">A sonification of Kepler space telescope star data Winton, Riley J.; Gable, Thomas M.; Schuett, Jonathan; Walker, Bruce N. A performing artist group interested in including a sonification of star data from NASA\\ufffd\\ufffd\\ufffds Kepler space telescope in their next album release approached the Georgia Tech Sonification Lab for assistance in the process. The artists had few constraints for the authors other than wanting the end product to be true to the data, and a musically appealing \\ufffd\\ufffd\\ufffdheavenly\\ufffd\\ufffd\\ufffd sound. Several sonifications of the data were created using various techniques, each resulting in a different sounding representation of the Kepler data. The details of this process are discussed in this poster. Ultimately, the researchers were able to produce the desired sounds via sound synthesis, and the artists plan to incorporate them into their next album release. Presented at the 18th International Conference on Auditory Display (ICAD2012) on June 18-21, 2012  in Atlanta, Georgia.; Reprinted by permission of the International Community for Auditory Display, http://www.icad.org. </summary> <dc:date>2012-06-01T00:00:00Z</dc:date> <dc:creator>Winton, Riley J.</dc:creator> <dc:creator>Gable, Thomas M.</dc:creator> <dc:creator>Schuett, Jonathan</dc:creator> <dc:creator>Walker, Bruce N.</dc:creator> <dc:description>A performing artist group interested in including a sonification of star data from NASA\\ufffd\\ufffd\\ufffds Kepler space telescope in their next album release approached the Georgia Tech Sonification Lab for assistance in the process. The artists had few constraints for the authors other than wanting the end product to be true to the data, and a musically appealing \\ufffd\\ufffd\\ufffdheavenly\\ufffd\\ufffd\\ufffd sound. Several sonifications of the data were created using various techniques, each resulting in a different sounding representation of the Kepler data. The details of this process are discussed in this poster. Ultimately, the researchers were able to produce the desired sounds via sound synthesis, and the artists plan to incorporate them into their next album release.</dc:description> </entry> <entry> <title>Perceptual effects of auditory information about own and other movements</title> <link href=\"http://hdl.handle.net/1853/44443\" rel=\"alternate\"/> <author> <name>Schmitz, Gerd</name> </author> <author> <name>Effenberg, Alfred O.</name> </author> <id>http://hdl.handle.net/1853/44443</id> <updated>2012-08-03T18:32:56Z</updated> <published>2012-06-01T00:00:00Z</published> <summary type=\"text\">Perceptual effects of auditory information about own and other movements Schmitz, Gerd; Effenberg, Alfred O. In sport accurate predictions of other persons\\ufffd\\ufffd\\ufffd movements are essential. Former studies have shown that predictions can be enhanced by mapping movements onto sound (sonification) and providing audiovisual feedback [1]. The present study investigated behavioral mechanisms of movement sonification and scrutinized whether effects of own movements and those of other persons can be predicted just by listening to them. Eight athletes heard sonifications of an indoor rower and quantified resulting velocities of a virtual boat. Although boat velocity was not mapped onto sound directly, it explained subjects\\ufffd\\ufffd\\ufffd quantifications by regression analysis (R2 = 0.80) significantly better than the directly sonified amplitude and force parameters. Thus perception of boat velocity might have emerged from those sonifications. Predictions of effects of unknown movements were above chance level and as good as predictions of own movements. Furthermore athletes were able to identify their own technique among others (d\\ufffd\\ufffd\\ufffd = 0.47 \\ufffd\\ufffd 0.43). The results confirm large perceptual effects of auditory feedback and - most importantly - suggest that movement sonification can address central motor representations just by listening to it. Therefore not only predictability but also synchronization with other persons\\ufffd\\ufffd\\ufffd movements might be supported. Presented at the 18th International Conference on Auditory Display (ICAD2012) on June 18-21, 2012  in Atlanta, Georgia.; Reprinted by permission of the International Community for Auditory Display, http://www.icad.org. </summary> <dc:date>2012-06-01T00:00:00Z</dc:date> <dc:creator>Schmitz, Gerd</dc:creator> <dc:creator>Effenberg, Alfred O.</dc:creator> <dc:description>In sport accurate predictions of other persons\\ufffd\\ufffd\\ufffd movements are essential. Former studies have shown that predictions can be enhanced by mapping movements onto sound (sonification) and providing audiovisual feedback [1]. The present study investigated behavioral mechanisms of movement sonification and scrutinized whether effects of own movements and those of other persons can be predicted just by listening to them. Eight athletes heard sonifications of an indoor rower and quantified resulting velocities of a virtual boat. Although boat velocity was not mapped onto sound directly, it explained subjects\\ufffd\\ufffd\\ufffd quantifications by regression analysis (R2 = 0.80) significantly better than the directly sonified amplitude and force parameters. Thus perception of boat velocity might have emerged from those sonifications. Predictions of effects of unknown movements were above chance level and as good as predictions of own movements. Furthermore athletes were able to identify their own technique among others (d\\ufffd\\ufffd\\ufffd = 0.47 \\ufffd\\ufffd 0.43). The results confirm large perceptual effects of auditory feedback and - most importantly - suggest that movement sonification can address central motor representations just by listening to it. Therefore not only predictability but also synchronization with other persons\\ufffd\\ufffd\\ufffd movements might be supported.</dc:description> </entry> <entry> <title>Exploring 3D audio for brain sonification</title> <link href=\"http://hdl.handle.net/1853/44442\" rel=\"alternate\"/> <author> <name>Schmele, Timothy</name> </author> <author> <name>Gomez, Imanol</name> </author> <id>http://hdl.handle.net/1853/44442</id> <updated>2012-08-03T18:32:56Z</updated> <published>2012-06-01T00:00:00Z</published> <summary type=\"text\">Exploring 3D audio for brain sonification Schmele, Timothy; Gomez, Imanol Brain activity data, measured by functional Magnetic Resonance Imaging (fMRI), produces extremely high dimensional, sparse and noisy signals which are difficult to visualize, monitor and analyze. The use of spatial music can be particularly appropriate to represent its contained patterns. The literature describes several research done on sonifying neuroimaging data as well as different techniques to use spatialization as a musical language. In this paper, we discuss an artistic approach to fMRI sonification exploiting new compositional paradigms in spatial music. There fore, we have consider the brain activity as audio base material of a the spatial musical composition. Our approach attempts to explore the aesthetic potential of brain sonification not by transforming the data beyond the recognizable, and presenting the data as direct as possible. Presented at the 18th International Conference on Auditory Display (ICAD2012) on June 18-21, 2012  in Atlanta, Georgia.; Reprinted by permission of the International Community for Auditory Display, http://www.icad.org. </summary> <dc:date>2012-06-01T00:00:00Z</dc:date> <dc:creator>Schmele, Timothy</dc:creator> <dc:creator>Gomez, Imanol</dc:creator> <dc:description>Brain activity data, measured by functional Magnetic Resonance Imaging (fMRI), produces extremely high dimensional, sparse and noisy signals which are difficult to visualize, monitor and analyze. The use of spatial music can be particularly appropriate to represent its contained patterns. The literature describes several research done on sonifying neuroimaging data as well as different techniques to use spatialization as a musical language. In this paper, we discuss an artistic approach to fMRI sonification exploiting new compositional paradigms in spatial music. There fore, we have consider the brain activity as audio base material of a the spatial musical composition. Our approach attempts to explore the aesthetic potential of brain sonification not by transforming the data beyond the recognizable, and presenting the data as direct as possible.</dc:description> </entry> <entry> <title>Sonifying ECoG seizure data with overtone mapping: a strategy for creating auditory gestalt from correlated multichannel data</title> <link href=\"http://hdl.handle.net/1853/44445\" rel=\"alternate\"/> <author> <name>Terasawa, Hiroko</name> </author> <author> <name>Parvizi, Josef</name> </author> <author> <name>Chafe, Chris</name> </author> <id>http://hdl.handle.net/1853/44445</id> <updated>2012-08-03T18:32:56Z</updated> <published>2012-06-01T00:00:00Z</published> <summary type=\"text\">Sonifying ECoG seizure data with overtone mapping: a strategy for creating auditory gestalt from correlated multichannel data Terasawa, Hiroko; Parvizi, Josef; Chafe, Chris This paper introduces a mapping method, harmonic mapping, which projects multichannel time-series data onto a harmonic series structure. Because of the common fate effect of gestalt principle, correlated signals are perceived as a unity, while uncorrelated signals are perceived segregated. This method is first examined with sonification of simple, generic datasets. Then, harmonic mapping is applied to sonification of an ECoG data of an epileptic seizure episode. The relationship between the gestalt formation the correlation in the data across channels is discussed in detail using a 16 channels reduced dataset. Finally, sonification of a 56 channel ECoG dataset is provided to demonstrate the advantage of the harmonic mapping. Presented at the 18th International Conference on Auditory Display (ICAD2012) on June 18-21, 2012  in Atlanta, Georgia.; Reprinted by permission of the International Community for Auditory Display, http://www.icad.org. </summary> <dc:date>2012-06-01T00:00:00Z</dc:date> <dc:creator>Terasawa, Hiroko</dc:creator> <dc:creator>Parvizi, Josef</dc:creator> <dc:creator>Chafe, Chris</dc:creator> <dc:description>This paper introduces a mapping method, harmonic mapping, which projects multichannel time-series data onto a harmonic series structure. Because of the common fate effect of gestalt principle, correlated signals are perceived as a unity, while uncorrelated signals are perceived segregated. This method is first examined with sonification of simple, generic datasets. Then, harmonic mapping is applied to sonification of an ECoG data of an epileptic seizure episode. The relationship between the gestalt formation the correlation in the data across channels is discussed in detail using a 16 channels reduced dataset. Finally, sonification of a 56 channel ECoG dataset is provided to demonstrate the advantage of the harmonic mapping.</dc:description> </entry> </feed> ", 
    "identity": {
        "subtype": "", 
        "is_error": false, 
        "version": "", 
        "protocol": "", 
        "language": "", 
        "service": "", 
        "has_dataset": false, 
        "has_metadata": false
    }, 
    "digest": "7b0e4d9c3b823579d38482c7bacb2d97", 
    "source_url": "https://smartech.gatech.edu/feed/atom_1.0/1853/44394"
}