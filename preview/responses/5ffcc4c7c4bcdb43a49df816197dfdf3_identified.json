{
    "content": "<?xml version=\"1.0\" encoding=\"UTF-8\"?> <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns=\"http://purl.org/rss/1.0/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\"> <channel rdf:about=\"http://hdl.handle.net/1853/50701\"> <title>International Conference on Auditory Display, 1994</title> <link>http://hdl.handle.net/1853/50701</link> <description/> <items> <rdf:Seq> <rdf:li rdf:resource=\"http://hdl.handle.net/1853/50884\"/> <rdf:li rdf:resource=\"http://hdl.handle.net/1853/50883\"/> <rdf:li rdf:resource=\"http://hdl.handle.net/1853/50882\"/> <rdf:li rdf:resource=\"http://hdl.handle.net/1853/50881\"/> </rdf:Seq> </items> <dc:date>2015-03-12T19:31:03Z</dc:date> </channel> <item rdf:about=\"http://hdl.handle.net/1853/50884\"> <title>Voice annotation of visual representations in computer-mediated collaborative learning</title> <link>http://hdl.handle.net/1853/50884</link> <description>Voice annotation of visual representations in computer-mediated collaborative learning Steeples, Christine New computer-based communications technologies-such as electronic conferencing, electronic mail, communal hypertext, and communal hypermedia databases-make it possible for people to collaborate in their learning, even when separated from one another in space and time. The development of computer environments supporting collaborative learning in enriched ways provides the goal and context for the research outlined here. This chapter explores the potential value of voice annotations in computer-mediated collaborative learning. Some purposes for voice annotations are examined. Findy an outline of a small-scale study where voice annotations have been used to support collaborative writing processes is given. Presented at 2nd International Conference on Auditory Display (ICAD), Santa Fe, New Mexico, November 7-9, 1994. </description> <dc:date>1994-11-01T00:00:00Z</dc:date> <dc:creator>Steeples, Christine</dc:creator> <dc:description>New computer-based communications technologies-such as electronic conferencing, electronic mail, communal hypertext, and communal hypermedia databases-make it possible for people to collaborate in their learning, even when separated from one another in space and time. The development of computer environments supporting collaborative learning in enriched ways provides the goal and context for the research outlined here. This chapter explores the potential value of voice annotations in computer-mediated collaborative learning. Some purposes for voice annotations are examined. Findy an outline of a small-scale study where voice annotations have been used to support collaborative writing processes is given.</dc:description> </item> <item rdf:about=\"http://hdl.handle.net/1853/50883\"> <title>Using virtual environment technology to present a digital sound library</title> <link>http://hdl.handle.net/1853/50883</link> <description>Using virtual environment technology to present a digital sound library Whitehead, John F Digital sound libraries of today can be difficult to navigate. For example, sampling keyboards often display only the currently selected sound, and it is not easy to know the adjacent sounds or how to find desired ones. Large libraries stored on compact disc may be arranged in an alphabetical file structure, documented by descriptions that do not represent their sounds well. These digital sound libraries are often difficult to scan and audition easily because they are not organized according to sonic qualities. The user does not browse the database directly, but via its text representation. The user must audition sounds one at a time, must remember previous sounds when comparing the current one, and has little sense of the total available selection. These difficulties can detract from the spontaneity and creativity of the user. Presented at 2nd International Conference on Auditory Display (ICAD), Santa Fe, New Mexico, November 7-9, 1994. </description> <dc:date>1994-11-01T00:00:00Z</dc:date> <dc:creator>Whitehead, John F</dc:creator> <dc:description>Digital sound libraries of today can be difficult to navigate. For example, sampling keyboards often display only the currently selected sound, and it is not easy to know the adjacent sounds or how to find desired ones. Large libraries stored on compact disc may be arranged in an alphabetical file structure, documented by descriptions that do not represent their sounds well. These digital sound libraries are often difficult to scan and audition easily because they are not organized according to sonic qualities. The user does not browse the database directly, but via its text representation. The user must audition sounds one at a time, must remember previous sounds when comparing the current one, and has little sense of the total available selection. These difficulties can detract from the spontaneity and creativity of the user.</dc:description> </item> <item rdf:about=\"http://hdl.handle.net/1853/50882\"> <title>Using audio windows to analyze music</title> <link>http://hdl.handle.net/1853/50882</link> <description>Using audio windows to analyze music Cohen, Michael Alternative nonimmersive perspectives enable new paradigms of perception, especially in the context of frames-of-reference for musical audition and groupware. \"maw,\" acronymic for multidimensional audio windows, is an application for manipulating sound sources and sinks in virtual rooms, featuring an exocentric graphical interface driving an egocentric audio backend. Listening to sound presented in such a spatial fashion is as different from conventional stereo mixes as sculpture is from painting. A schizophrenic existence suggests sonic (analytic) cubism, presenting multiple acoustic perspectives simultaneously. Clusters can be used to hierarchically group related mixels together. New interaction modalities are enabled by this sort of perceptual aggression and liquid perspective. In particular, virtual concerts may be \"broken down\" by individuals or groups. Presented at 2nd International Conference on Auditory Display (ICAD), Santa Fe, New Mexico, November 7-9, 1994. </description> <dc:date>1994-11-01T00:00:00Z</dc:date> <dc:creator>Cohen, Michael</dc:creator> <dc:description>Alternative nonimmersive perspectives enable new paradigms of perception, especially in the context of frames-of-reference for musical audition and groupware. \"maw,\" acronymic for multidimensional audio windows, is an application for manipulating sound sources and sinks in virtual rooms, featuring an exocentric graphical interface driving an egocentric audio backend. Listening to sound presented in such a spatial fashion is as different from conventional stereo mixes as sculpture is from painting. A schizophrenic existence suggests sonic (analytic) cubism, presenting multiple acoustic perspectives simultaneously. Clusters can be used to hierarchically group related mixels together. New interaction modalities are enabled by this sort of perceptual aggression and liquid perspective. In particular, virtual concerts may be \"broken down\" by individuals or groups.</dc:description> </item> <item rdf:about=\"http://hdl.handle.net/1853/50881\"> <title>Using additive sound synthesis to analyze simplicial complexes</title> <link>http://hdl.handle.net/1853/50881</link> <description>Using additive sound synthesis to analyze simplicial complexes Axen, Ulrike; Choi, Insook We present a new technique for traversing simplicia1 complexes and producing sounds from the output of this traversal. The traversal algorithm was invented in order to extract temporal information from static geometric structures; this information is used as input to a sound synthesis algorithm. A systematic traversal of complexes and associating data to parameters of sound synthesis has many possible applications: the analysis of objects of dimension 4 or higher, exploration of large data sets and music composition. In this paper we limit ourselves to bdimensional objects and present the experimental results from the first phase of our work. Presented at 2nd International Conference on Auditory Display (ICAD), Santa Fe, New Mexico, November 7-9, 1994. </description> <dc:date>1994-11-01T00:00:00Z</dc:date> <dc:creator>Axen, Ulrike</dc:creator> <dc:creator>Choi, Insook</dc:creator> <dc:description>We present a new technique for traversing simplicia1 complexes and producing sounds from the output of this traversal. The traversal algorithm was invented in order to extract temporal information from static geometric structures; this information is used as input to a sound synthesis algorithm. A systematic traversal of complexes and associating data to parameters of sound synthesis has many possible applications: the analysis of objects of dimension 4 or higher, exploration of large data sets and music composition. In this paper we limit ourselves to bdimensional objects and present the experimental results from the first phase of our work.</dc:description> </item> </rdf:RDF> ", 
    "identity": {
        "subtype": "dataset", 
        "is_error": false, 
        "version": "", 
        "protocol": "RDF", 
        "language": "", 
        "service": "", 
        "has_dataset": false, 
        "has_metadata": false
    }, 
    "digest": "5ffcc4c7c4bcdb43a49df816197dfdf3", 
    "source_url": "https://smartech.gatech.edu/feed/rss_1.0/1853/50701"
}