{
    "content": "<?xml version=\"1.0\" encoding=\"UTF-8\"?> <feed xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns=\"http://www.w3.org/2005/Atom\"> <title>International Conference on Auditory Display, 2000</title> <link href=\"http://hdl.handle.net/1853/50204\" rel=\"alternate\"/> <subtitle/> <id>http://hdl.handle.net/1853/50204</id> <updated>2015-03-12T19:35:12Z</updated> <dc:date>2015-03-12T19:35:12Z</dc:date> <entry> <title>Websound: A generic web sonification tool, and its application to an auditory web browser for blind and visually impaired users</title> <link href=\"http://hdl.handle.net/1853/50688\" rel=\"alternate\"/> <author> <name>Petrucci, Lori Stefano</name> </author> <author> <name>Harth, Eric</name> </author> <author> <name>Roth, Patrick</name> </author> <author> <name>Assimacopoulos, Andre</name> </author> <author> <name>Pun, Thierry</name> </author> <id>http://hdl.handle.net/1853/50688</id> <updated>2014-01-22T08:31:48Z</updated> <published>2000-04-01T00:00:00Z</published> <summary type=\"text\">Websound: A generic web sonification tool, and its application to an auditory web browser for blind and visually impaired users Petrucci, Lori Stefano; Harth, Eric; Roth, Patrick; Assimacopoulos, Andre; Pun, Thierry The inherent visual nature of Internet browsers makes the Web inaccessible to the visually impaired. Although several nonvisual browsers have been developed, they usually transform the visual content of HTML documents into textual information only, that can be rendered by a text-to-speech converter or a Braille device. The loss of spatial layout and of textual attributes should be avoided since they often bear visually important information. Moreover, typical non-visual Internet browsers do not allow visually impaired and sighted individuals to easily work together using the same environment. This paper presents WebSound, a new generic Web sonification tool and its application to a 3D audio augmented Internet browser (Internet Explorer 5.0) developed by the Computer Vision Group at the University of Geneva. Presented of the 6th International Conference on Auditory Display (ICAD), Atlanta, GA, April 2-5, 2000 </summary> <dc:date>2000-04-01T00:00:00Z</dc:date> <dc:creator>Petrucci, Lori Stefano</dc:creator> <dc:creator>Harth, Eric</dc:creator> <dc:creator>Roth, Patrick</dc:creator> <dc:creator>Assimacopoulos, Andre</dc:creator> <dc:creator>Pun, Thierry</dc:creator> <dc:description>The inherent visual nature of Internet browsers makes the Web inaccessible to the visually impaired. Although several nonvisual browsers have been developed, they usually transform the visual content of HTML documents into textual information only, that can be rendered by a text-to-speech converter or a Braille device. The loss of spatial layout and of textual attributes should be avoided since they often bear visually important information. Moreover, typical non-visual Internet browsers do not allow visually impaired and sighted individuals to easily work together using the same environment. This paper presents WebSound, a new generic Web sonification tool and its application to a 3D audio augmented Internet browser (Internet Explorer 5.0) developed by the Computer Vision Group at the University of Geneva.</dc:description> </entry> <entry> <title>The NAVE: Design and implementation of a 3D audio system for a low cost spatially immersive display</title> <link href=\"http://hdl.handle.net/1853/50687\" rel=\"alternate\"/> <author> <name>Wilson, Jeff</name> </author> <author> <name>Pair, Jarrell</name> </author> <id>http://hdl.handle.net/1853/50687</id> <updated>2014-01-22T08:31:56Z</updated> <published>2000-04-01T00:00:00Z</published> <summary type=\"text\">The NAVE: Design and implementation of a 3D audio system for a low cost spatially immersive display Wilson, Jeff; Pair, Jarrell The NAVE is a low cost spatially immersive display system developed by the Georgia Tech Virtual Environments Group. The NAVE audio environment uses two independent speaker systems driven by a dedicated audio PC with two sound cards. The speakers are amplified by four Pioneer 200 watt receivers. The primary system utilizes 4 Bose Acoustimass speakers to create a three dimensional soundfield. A Sound Blaster LIVE! audio card is used for audio spatialization. The secondary system steers bass audio content across four zones embedded in the NAVE floor. Bass audio is moved within the zones using a Diamond Monster Sound MX200 PC sound card. Bass vibration effects are maximized by six Aura Bass Shaker Pro's along with seat mounted subwoofer speakers. The steerable bass audio system can be used to create audio-tactile effects for simulating the vibration of vehicles, thunder, explosions, shockwaves, and earthquakes. A custom real-time audio effects application programming interface (API) allows virtual environment designers to attach sounds to graphic entities. The spatial position of sounds are synchronized with the position of their associated graphical objects. The API is also capable of reproducing real world phenomena including reverberation and doppler effects. Presented of the 6th International Conference on Auditory Display (ICAD), Atlanta, GA, April 2-5, 2000 </summary> <dc:date>2000-04-01T00:00:00Z</dc:date> <dc:creator>Wilson, Jeff</dc:creator> <dc:creator>Pair, Jarrell</dc:creator> <dc:description>The NAVE is a low cost spatially immersive display system developed by the Georgia Tech Virtual Environments Group. The NAVE audio environment uses two independent speaker systems driven by a dedicated audio PC with two sound cards. The speakers are amplified by four Pioneer 200 watt receivers. The primary system utilizes 4 Bose Acoustimass speakers to create a three dimensional soundfield. A Sound Blaster LIVE! audio card is used for audio spatialization. The secondary system steers bass audio content across four zones embedded in the NAVE floor. Bass audio is moved within the zones using a Diamond Monster Sound MX200 PC sound card. Bass vibration effects are maximized by six Aura Bass Shaker Pro's along with seat mounted subwoofer speakers. The steerable bass audio system can be used to create audio-tactile effects for simulating the vibration of vehicles, thunder, explosions, shockwaves, and earthquakes. A custom real-time audio effects application programming interface (API) allows virtual environment designers to attach sounds to graphic entities. The spatial position of sounds are synchronized with the position of their associated graphical objects. The API is also capable of reproducing real world phenomena including reverberation and doppler effects.</dc:description> </entry> <entry> <title>Tools for auditory display research</title> <link href=\"http://hdl.handle.net/1853/50686\" rel=\"alternate\"/> <author> <name>Tucker, Timothy</name> </author> <author> <name>Mann, David</name> </author> <author> <name>Wilson, Wilard</name> </author> <id>http://hdl.handle.net/1853/50686</id> <updated>2014-01-22T08:31:55Z</updated> <published>2000-04-01T00:00:00Z</published> <summary type=\"text\">Tools for auditory display research Tucker, Timothy; Mann, David; Wilson, Wilard TDT will demonstrate its Power sDAC Convolver and RP2 Real-Time Processor systems for generating real-time 3D auditory displays based on head-related transfer functions (HRTFs) and head-tracker data. These tools are designed to support research into the efficacy of auditory display techniques by giving the user complete control over signal generation, presentation, and dynamic updating of position-specific HRTFs. They allow display of multiple acoustic images and creation of reverberant models. TDT has also created a library of HRTF's measured from over 100 subjects in a custom anechoic chamber with a speaker mounted on a robot arm. Presented of the 6th International Conference on Auditory Display (ICAD), Atlanta, GA, April 2-5, 2000 </summary> <dc:date>2000-04-01T00:00:00Z</dc:date> <dc:creator>Tucker, Timothy</dc:creator> <dc:creator>Mann, David</dc:creator> <dc:creator>Wilson, Wilard</dc:creator> <dc:description>TDT will demonstrate its Power sDAC Convolver and RP2 Real-Time Processor systems for generating real-time 3D auditory displays based on head-related transfer functions (HRTFs) and head-tracker data. These tools are designed to support research into the efficacy of auditory display techniques by giving the user complete control over signal generation, presentation, and dynamic updating of position-specific HRTFs. They allow display of multiple acoustic images and creation of reverberant models. TDT has also created a library of HRTF's measured from over 100 subjects in a custom anechoic chamber with a speaker mounted on a robot arm.</dc:description> </entry> <entry> <title>The effect of earcons on reaction times and error-rates in a dual-task vs. a single-task experiment</title> <link href=\"http://hdl.handle.net/1853/50685\" rel=\"alternate\"/> <author> <name>Lemmens, Paul M.C</name> </author> <author> <name>Bussemakers, Myra P</name> </author> <author> <name>de Haan, Abraham</name> </author> <id>http://hdl.handle.net/1853/50685</id> <updated>2014-01-22T08:31:54Z</updated> <published>2000-04-01T00:00:00Z</published> <summary type=\"text\">The effect of earcons on reaction times and error-rates in a dual-task vs. a single-task experiment Lemmens, Paul M.C; Bussemakers, Myra P; de Haan, Abraham An experiment with two picture categorization tasks with auditory distracters containing redundant information was carried out to investigate the effects distracters, in this case earcons, have on categorization. In the first task participants had to carry out an extra mental addition task. The secondary task consisted of just the categorization. The dual-task situation was expected to lead to longer reaction times and more errors than the single-task situation, possibly providing new insights when analyzed. The results confirmed previous experimental results and indicated that significantly fewer errors were made in conditions in which the earcons contained relevant redundant information, compared to conditions with no relevant redundant information. Presented of the 6th International Conference on Auditory Display (ICAD), Atlanta, GA, April 2-5, 2000 </summary> <dc:date>2000-04-01T00:00:00Z</dc:date> <dc:creator>Lemmens, Paul M.C</dc:creator> <dc:creator>Bussemakers, Myra P</dc:creator> <dc:creator>de Haan, Abraham</dc:creator> <dc:description>An experiment with two picture categorization tasks with auditory distracters containing redundant information was carried out to investigate the effects distracters, in this case earcons, have on categorization. In the first task participants had to carry out an extra mental addition task. The secondary task consisted of just the categorization. The dual-task situation was expected to lead to longer reaction times and more errors than the single-task situation, possibly providing new insights when analyzed. The results confirmed previous experimental results and indicated that significantly fewer errors were made in conditions in which the earcons contained relevant redundant information, compared to conditions with no relevant redundant information.</dc:description> </entry> </feed> ", 
    "identity": {
        "subtype": "", 
        "is_error": false, 
        "version": "", 
        "protocol": "", 
        "language": "", 
        "service": "", 
        "has_dataset": false, 
        "has_metadata": false
    }, 
    "digest": "8ab6908185f39673363b932629778bff", 
    "source_url": "https://smartech.gatech.edu/feed/atom_1.0/1853/50204"
}