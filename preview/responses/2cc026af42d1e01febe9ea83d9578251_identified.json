{
    "content": "<?xml version=\"1.0\" encoding=\"UTF-8\"?> <feed xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns=\"http://www.w3.org/2005/Atom\"> <title>International Conference on Auditory Display (ICAD)</title> <link href=\"http://hdl.handle.net/1853/49750\" rel=\"alternate\"/> <subtitle/> <id>http://hdl.handle.net/1853/49750</id> <updated>2015-03-12T15:29:43Z</updated> <dc:date>2015-03-12T15:29:43Z</dc:date> <entry> <title>Comparing Auditory Stimuli for Sleep Enhancement: Mimicking a Sleeping Situation</title> <link href=\"http://hdl.handle.net/1853/52132\" rel=\"alternate\"/> <author> <name>Leminen, Miika</name> </author> <author> <name>Ahonen, Lauri</name> </author> <author> <name>Gr\\ufffd\\ufffdhn, Matti</name> </author> <author> <name>Huotilainen, Minna</name> </author> <author> <name>Paunio, Tiina</name> </author> <author> <name>Virkkala, Jussi</name> </author> <id>http://hdl.handle.net/1853/52132</id> <updated>2014-09-05T07:32:17Z</updated> <published>2014-06-01T00:00:00Z</published> <summary type=\"text\">Comparing Auditory Stimuli for Sleep Enhancement: Mimicking a Sleeping Situation Leminen, Miika; Ahonen, Lauri; Gr\\ufffd\\ufffdhn, Matti; Huotilainen, Minna; Paunio, Tiina; Virkkala, Jussi Recently, two research groups have reported that the depth&#13; and/or duration of Slow Wave Sleep (SWS) can be increased by&#13; playing short sounds with approximately 1 second intervals during&#13; or prior to SWS. These studies have used sounds with neutral&#13; or negative valence: sinusoidal I -kHz tones or short pink noise&#13; bursts. Since music therapy research shows beneficial effects of&#13; pleasant, natural sounds and music, the sounds in the experiments&#13; may have been suboptimal. Thus, we aimed at choosing optimal&#13; sounds such that they could be used in increasing the depth or duration&#13; of SWS taking into account both the need of fast rise times,&#13; short duration, and pleasantness. Here we report results of a listening&#13; test mimicking a sleeping situation in which the subjects&#13; compared how pleasant, relaxing, and image-evoking they found&#13; 3 natural, short instrument sounds with fast rise times compared&#13; to a short pink noise burst used in the previous experiments. The&#13; natural sounds were selected from our previous listening test as&#13; the most pleasant ones. The results will be used as the basis for&#13; choosing the optimal sounds for the sleep studies. Presented at the 20th International Conference on Auditory Display (ICAD2014), June 22-25, 2014, New York, NY. </summary> <dc:date>2014-06-01T00:00:00Z</dc:date> <dc:creator>Leminen, Miika</dc:creator> <dc:creator>Ahonen, Lauri</dc:creator> <dc:creator>Gr\\ufffd\\ufffdhn, Matti</dc:creator> <dc:creator>Huotilainen, Minna</dc:creator> <dc:creator>Paunio, Tiina</dc:creator> <dc:creator>Virkkala, Jussi</dc:creator> <dc:description>Recently, two research groups have reported that the depth&#13; and/or duration of Slow Wave Sleep (SWS) can be increased by&#13; playing short sounds with approximately 1 second intervals during&#13; or prior to SWS. These studies have used sounds with neutral&#13; or negative valence: sinusoidal I -kHz tones or short pink noise&#13; bursts. Since music therapy research shows beneficial effects of&#13; pleasant, natural sounds and music, the sounds in the experiments&#13; may have been suboptimal. Thus, we aimed at choosing optimal&#13; sounds such that they could be used in increasing the depth or duration&#13; of SWS taking into account both the need of fast rise times,&#13; short duration, and pleasantness. Here we report results of a listening&#13; test mimicking a sleeping situation in which the subjects&#13; compared how pleasant, relaxing, and image-evoking they found&#13; 3 natural, short instrument sounds with fast rise times compared&#13; to a short pink noise burst used in the previous experiments. The&#13; natural sounds were selected from our previous listening test as&#13; the most pleasant ones. The results will be used as the basis for&#13; choosing the optimal sounds for the sleep studies.</dc:description> </entry> <entry> <title>Extended Abstract: Using Vocal-Based Sounds to Represent Sentiment in Complex Event Processing</title> <link href=\"http://hdl.handle.net/1853/52105\" rel=\"alternate\"/> <author> <name>Rimland, Jeff</name> </author> <author> <name>Ballora, Mark</name> </author> <id>http://hdl.handle.net/1853/52105</id> <updated>2014-09-05T07:32:14Z</updated> <published>2014-06-01T00:00:00Z</published> <summary type=\"text\">Extended Abstract: Using Vocal-Based Sounds to Represent Sentiment in Complex Event Processing Rimland, Jeff; Ballora, Mark There is an intricate and evolving relationship between&#13; sonification and Complex Event Processing (CEP) for&#13; improved situational awareness. In a paper presented at ICAD&#13; 2013 [1], we introduced a series of techniques using CEP for&#13; simultaneous sonification of both quantitative \\ufffd\\ufffd\\ufffdhard\\ufffd\\ufffd\\ufffd data and&#13; human-derived \\ufffd\\ufffd\\ufffdsoft\\ufffd\\ufffd\\ufffd data in the context of assistive&#13; technology. The connection of CEP and sonification was&#13; explored further in the context of a severe weather tracker that&#13; relies on fusion of quantitative (sensor-based) weather data&#13; along with human observations about storms and related&#13; conditions [2]. An area of shortcoming in both of these earlier&#13; works was the difficulty in creating sounds that represented&#13; human sentiment about observed conditions (e.g. unanticipated&#13; obstacles for a blind person crossing a busy street, or&#13; impending dangerous weather conditions) in a format that&#13; enabled intuitive listening for improved situational awareness.&#13; This extended abstract provides an update on that continuing&#13; research by representing human sentiment data, via the use of&#13; vocal synthesis that is driven by Complex Event Processing. Presented at the 20th International Conference on Auditory Display (ICAD2014), June 22-25, 2014, New York, NY. </summary> <dc:date>2014-06-01T00:00:00Z</dc:date> <dc:creator>Rimland, Jeff</dc:creator> <dc:creator>Ballora, Mark</dc:creator> <dc:description>There is an intricate and evolving relationship between&#13; sonification and Complex Event Processing (CEP) for&#13; improved situational awareness. In a paper presented at ICAD&#13; 2013 [1], we introduced a series of techniques using CEP for&#13; simultaneous sonification of both quantitative \\ufffd\\ufffd\\ufffdhard\\ufffd\\ufffd\\ufffd data and&#13; human-derived \\ufffd\\ufffd\\ufffdsoft\\ufffd\\ufffd\\ufffd data in the context of assistive&#13; technology. The connection of CEP and sonification was&#13; explored further in the context of a severe weather tracker that&#13; relies on fusion of quantitative (sensor-based) weather data&#13; along with human observations about storms and related&#13; conditions [2]. An area of shortcoming in both of these earlier&#13; works was the difficulty in creating sounds that represented&#13; human sentiment about observed conditions (e.g. unanticipated&#13; obstacles for a blind person crossing a busy street, or&#13; impending dangerous weather conditions) in a format that&#13; enabled intuitive listening for improved situational awareness.&#13; This extended abstract provides an update on that continuing&#13; research by representing human sentiment data, via the use of&#13; vocal synthesis that is driven by Complex Event Processing.</dc:description> </entry> <entry> <title>iEAR: Immersive Environmental Audio for Photorealistic Panoramas</title> <link href=\"http://hdl.handle.net/1853/52104\" rel=\"alternate\"/> <author> <name>Riker, Paul</name> </author> <author> <name>Acevedo, Daniel</name> </author> <id>http://hdl.handle.net/1853/52104</id> <updated>2014-09-05T07:32:14Z</updated> <published>2014-06-01T00:00:00Z</published> <summary type=\"text\">iEAR: Immersive Environmental Audio for Photorealistic Panoramas Riker, Paul; Acevedo, Daniel This paper presents iEAR, a flexible spatial audio rendering tool for use with photorealistic monoscopic and stereoscopic&#13; panoramas across various display systems. iEAR allows users to easily present multichannel audio scenes over variable&#13; speaker arrangements, while maintaining tight integration with the corresponding visual elements of the display media. Built in the Max/MSP Audio Programming Environment, iEAR&#13; utilizes well-established panning methods to accommodate a&#13; wide range of speaker configurations. Audio scene orientation&#13; is tied to the visual scene using an OSC connection with the visualization software, allowing users to render and spatialize&#13; multichannel environmental audio recordings in tandem with&#13; the changing perspective in the visual scene. Presented at the 20th International Conference on Auditory Display (ICAD2014), June 22-25, 2014, New York, NY. </summary> <dc:date>2014-06-01T00:00:00Z</dc:date> <dc:creator>Riker, Paul</dc:creator> <dc:creator>Acevedo, Daniel</dc:creator> <dc:description>This paper presents iEAR, a flexible spatial audio rendering tool for use with photorealistic monoscopic and stereoscopic&#13; panoramas across various display systems. iEAR allows users to easily present multichannel audio scenes over variable&#13; speaker arrangements, while maintaining tight integration with the corresponding visual elements of the display media. Built in the Max/MSP Audio Programming Environment, iEAR&#13; utilizes well-established panning methods to accommodate a&#13; wide range of speaker configurations. Audio scene orientation&#13; is tied to the visual scene using an OSC connection with the visualization software, allowing users to render and spatialize&#13; multichannel environmental audio recordings in tandem with&#13; the changing perspective in the visual scene.</dc:description> </entry> <entry> <title>Sonification Synthesizer for Surface Electromyography</title> <link href=\"http://hdl.handle.net/1853/52103\" rel=\"alternate\"/> <author> <name>Peres, S. Camille</name> </author> <author> <name>Faisst, Cody</name> </author> <author> <name>Slota, Nathan</name> </author> <author> <name>Verona, Daniel</name> </author> <author> <name>Williams, Chase</name> </author> <author> <name>Ritchey, Paul</name> </author> <id>http://hdl.handle.net/1853/52103</id> <updated>2014-09-05T07:32:14Z</updated> <published>2014-06-01T00:00:00Z</published> <summary type=\"text\">Sonification Synthesizer for Surface Electromyography Peres, S. Camille; Faisst, Cody; Slota, Nathan; Verona, Daniel; Williams, Chase; Ritchey, Paul Surface electromyography (sEMG) is a means for measuring muscle activity. sEMG data are typically displayed graphically on a computer screen and while this can be a useful way to display the data, it is not always ideal. This extended abstract details the development of a sonification tool that allows users to sonify sEMG data in real-time. The tool will allow the user to independently control the sound of each channel, similar to a software synthesizer. Independent real-time control of each channel will allow the user to create sonification models, which are mappings of certain sounds to specific muscle groups. A prototype of the tool is currently being developed using SuperCollider in parallel with a Delsys Trigno Wireless sEMG system. This tool will allow users to easily explore various kinds of sEMG sonification models and test them for intuitiveness and effectiveness. Presented at the 20th International Conference on Auditory Display (ICAD2014), June 22-25, 2014, New York, NY. </summary> <dc:date>2014-06-01T00:00:00Z</dc:date> <dc:creator>Peres, S. Camille</dc:creator> <dc:creator>Faisst, Cody</dc:creator> <dc:creator>Slota, Nathan</dc:creator> <dc:creator>Verona, Daniel</dc:creator> <dc:creator>Williams, Chase</dc:creator> <dc:creator>Ritchey, Paul</dc:creator> <dc:description>Surface electromyography (sEMG) is a means for measuring muscle activity. sEMG data are typically displayed graphically on a computer screen and while this can be a useful way to display the data, it is not always ideal. This extended abstract details the development of a sonification tool that allows users to sonify sEMG data in real-time. The tool will allow the user to independently control the sound of each channel, similar to a software synthesizer. Independent real-time control of each channel will allow the user to create sonification models, which are mappings of certain sounds to specific muscle groups. A prototype of the tool is currently being developed using SuperCollider in parallel with a Delsys Trigno Wireless sEMG system. This tool will allow users to easily explore various kinds of sEMG sonification models and test them for intuitiveness and effectiveness.</dc:description> </entry> </feed> ", 
    "identity": {
        "subtype": "", 
        "is_error": false, 
        "version": "", 
        "protocol": "", 
        "language": "", 
        "service": "", 
        "has_dataset": false, 
        "has_metadata": false
    }, 
    "digest": "2cc026af42d1e01febe9ea83d9578251", 
    "source_url": "https://smartech.gatech.edu/feed/atom_1.0/1853/49750"
}